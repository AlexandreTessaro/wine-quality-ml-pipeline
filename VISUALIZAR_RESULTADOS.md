# üìä Como Visualizar os Resultados

Este guia mostra como visualizar os gr√°ficos e resultados gerados pelo pipeline.

## üìà Gr√°ficos Gerados

O projeto gera automaticamente dois gr√°ficos principais na pasta `results/`:

### 1. Compara√ß√£o de M√©tricas
**Arquivo**: `results/metric_comparison.png`

Este gr√°fico mostra a compara√ß√£o visual das tr√™s m√©tricas (Accuracy, Precision, F1-Score) entre os tr√™s modelos.

**Como visualizar**:
- Abra o arquivo `results/metric_comparison.png` em qualquer visualizador de imagens
- Ou execute no Python:
```python
from PIL import Image
Image.open('results/metric_comparison.png').show()
```

### 2. Matrizes de Confus√£o
**Arquivo**: `results/confusion_matrices.png`

Este gr√°fico mostra tr√™s matrizes de confus√£o (uma para cada modelo), permitindo visualizar onde cada modelo acerta e erra.

**Como visualizar**:
- Abra o arquivo `results/confusion_matrices.png` em qualquer visualizador de imagens
- Ou execute no Python:
```python
from PIL import Image
Image.open('results/confusion_matrices.png').show()
```

## üìã Tabela de Resultados

**Arquivo**: `results/model_results.csv`

Tabela CSV com os resultados num√©ricos de cada modelo.

**Como visualizar**:
```python
import pandas as pd
df = pd.read_csv('results/model_results.csv')
print(df.to_string(index=False))
```

**Ou abra diretamente** no Excel, Google Sheets ou qualquer editor de CSV.

## üéØ Resultados Num√©ricos

### Resumo dos Resultados

| Modelo | Accuracy | Precision | F1-Score |
|--------|----------|-----------|----------|
| **SVM** | **0.5714** | **0.5490** | 0.5306 |
| Random Forest | 0.5675 | 0.5357 | **0.5365** |
| Gradient Boosting | 0.5334 | 0.5116 | 0.5118 |

### Interpreta√ß√£o

- **SVM** teve o melhor desempenho em Accuracy (57.14%) e Precision (54.90%)
- **Random Forest** teve o melhor F1-Score (53.65%)
- Todos os modelos apresentaram desempenho similar, indicando robustez

## üîç An√°lise Detalhada

Para uma an√°lise mais detalhada, execute:

```bash
python src/evaluate.py
```

Isso mostrar√°:
- Relat√≥rios de classifica√ß√£o completos
- Matrizes de confus√£o detalhadas
- M√©tricas por classe
- An√°lise de erros

## üìä Visualiza√ß√£o Interativa (Opcional)

Para criar visualiza√ß√µes interativas, voc√™ pode usar:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Carregar resultados
df = pd.read_csv('results/model_results.csv')

# Criar gr√°fico customizado
plt.figure(figsize=(10, 6))
df_melted = df.melt(id_vars='Modelo', var_name='M√©trica', value_name='Score')
sns.barplot(data=df_melted, x='Modelo', y='Score', hue='M√©trica')
plt.title('Compara√ß√£o de Modelos')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## üñºÔ∏è Incluindo Gr√°ficos no README

Os gr√°ficos podem ser inclu√≠dos no README usando:

```markdown
![Compara√ß√£o de M√©tricas](results/metric_comparison.png)
![Matrizes de Confus√£o](results/confusion_matrices.png)
```

---

**Dica**: Os gr√°ficos s√£o gerados automaticamente quando voc√™ executa `python src/evaluate.py` ou `python main.py`.

